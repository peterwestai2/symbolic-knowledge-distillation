{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This script demonstrates how to generate from trained COMET distil models\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-difference",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "First, load the comet distil model to use\n",
    "\n",
    "Requires:\n",
    "    Download the full model/data directory. Point model_path to the model version\n",
    "    you would like to use. Point tokenizer path to the tokenizer you would like\n",
    "    to use.\n",
    "\n",
    "'''\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "\n",
    "# path to model directory\n",
    "model_path = 'downloaded/comet-distill-low/'\n",
    "# Point to a file containing config.json, merges.txt, added_tokens.json, vocab.json, \n",
    "# special_tokens_map.json, tokenizer_config.json\n",
    "#\n",
    "# NOTE: you may have to rename tokenizer_config.json to config.json\n",
    "tokenizer_path = 'downloaded/comet-distill-tokenizer'\n",
    "\n",
    "\n",
    "## load model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-conservative",
   "metadata": {},
   "source": [
    "# Generating on a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Generate an inference \n",
    "\n",
    "Requires: \n",
    "    Select an event, relation, and top_p if sampling (leaving it at 0. will \n",
    "    use greedy decoding)\n",
    "\n",
    "'''\n",
    "\n",
    "####\n",
    "# Parameters\n",
    "####\n",
    "\n",
    "# input event to generate for\n",
    "event = 'PersonX eats less'\n",
    "# one of ['xEffect','xAttr','xReact', 'xWant','xIntent', 'xNeed', 'HinderedBy']\n",
    "relation = 'xEffect'\n",
    "# top_p\n",
    "top_p = 0.0\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "# Generate, using parameters\n",
    "####\n",
    "\n",
    "inp_text = \"<head> {} </head> <relation> {} </relation> [GEN] \".format(event.strip(),relation)\n",
    "inp = tokenizer(inp_text)['input_ids']\n",
    "tokens_out = model.generate(torch.tensor(inp).view(1,-1),top_p=top_p, do_sample=top_p > 0.).tolist()[0]\n",
    "text_out = tokenizer.decode(tokens_out[len(inp):])\n",
    "\n",
    "## trim to sentence end and/or newline\n",
    "if '.' in text_out:\n",
    "    text_out = text_out[:text_out.index('.')]\n",
    "if '\\n' in text_out:\n",
    "    text_out = text_out[:text_out.index('\\n')]\n",
    "\n",
    "# finally, trim out only the inference\n",
    "inference = text_out\n",
    "\n",
    "\n",
    "####\n",
    "# Print Result\n",
    "####\n",
    "\n",
    "print('INPUT:\\n{}\\n========'.format(inp_text))\n",
    "print('INFERENCE:\\n{}\\n========'.format(inference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-mobility",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
